{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7fd5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Reez\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eef9c531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "DATA_DIR = './data'\n",
    "# Consider changing the output filename to indicate raw coordinates\n",
    "OUTPUT_CSV_FILE = 'landmark_data_raw_new.csv' # <-- Changed filename\n",
    "NUM_LANDMARKS = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57b85f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MediaPipe Initialization ---\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True,\n",
    "                       max_num_hands=1,\n",
    "                       min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b2666fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data collection from: ./data\n"
     ]
    }
   ],
   "source": [
    "# --- Data Collection ---\n",
    "all_rows = []\n",
    "processed_files = 0\n",
    "skipped_files = 0\n",
    "\n",
    "print(f\"Starting data collection from: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64f2fa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Headers: ['x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4', 'x5', 'y5', 'x6', 'y6', 'x7', 'y7', 'x8', 'y8', 'x9', 'y9', 'x10', 'y10', 'x11', 'y11', 'x12', 'y12', 'x13', 'y13', 'x14', 'y14', 'x15', 'y15', 'x16', 'y16', 'x17', 'y17', 'x18', 'y18', 'x19', 'y19', 'x20', 'y20', 'x21', 'y21', 'label']\n"
     ]
    }
   ],
   "source": [
    "# Define headers\n",
    "headers = []\n",
    "for i in range(1, NUM_LANDMARKS + 1):\n",
    "    headers.extend([f'x{i}', f'y{i}'])\n",
    "headers.append('label')\n",
    "\n",
    "print(f\"CSV Headers: {headers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82865fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing classes: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y']\n"
     ]
    }
   ],
   "source": [
    "# Iterate through classes\n",
    "class_names = sorted([d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))])\n",
    "class_names = [c for c in class_names if c.upper() not in ('J', 'Z')]\n",
    "print(f\"Processing classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97201749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class: A\n",
      " Found 200 images.\n",
      " Finished class A. Processed: 200, Skipped: 0\n",
      "\n",
      "Processing class: B\n",
      " Found 200 images.\n",
      " Finished class B. Processed: 200, Skipped: 0\n",
      "\n",
      "Processing class: C\n",
      " Found 200 images.\n",
      " Finished class C. Processed: 200, Skipped: 0\n",
      "\n",
      "Processing class: D\n",
      " Found 200 images.\n",
      " Finished class D. Processed: 200, Skipped: 0\n",
      "\n",
      "Processing class: E\n",
      " Found 200 images.\n",
      " Finished class E. Processed: 200, Skipped: 0\n",
      "\n",
      "Processing class: F\n",
      " Found 200 images.\n",
      " Finished class F. Processed: 200, Skipped: 0\n",
      "\n",
      "Processing class: G\n",
      " Found 200 images.\n",
      " Finished class G. Processed: 200, Skipped: 0\n",
      "\n",
      "Processing class: H\n",
      " Found 200 images.\n",
      " Finished class H. Processed: 200, Skipped: 0\n",
      "\n",
      "Processing class: I\n",
      " Found 200 images.\n",
      " Finished class I. Processed: 200, Skipped: 0\n",
      "\n",
      "Processing class: K\n",
      " Found 200 images.\n",
      " Finished class K. Processed: 200, Skipped: 0\n",
      "\n",
      "Processing class: L\n",
      " Found 200 images.\n",
      " Finished class L. Processed: 200, Skipped: 0\n",
      "\n",
      "Processing class: M\n",
      " Found 200 images.\n",
      " Finished class M. Processed: 200, Skipped: 0\n",
      "\n",
      "Processing class: N\n",
      " Found 200 images.\n",
      " Finished class N. Processed: 200, Skipped: 0\n",
      "\n",
      "Processing class: O\n",
      " Found 200 images.\n",
      " Finished class O. Processed: 200, Skipped: 0\n",
      "\n",
      "Processing class: P\n",
      " Found 200 images.\n",
      " Finished class P. Processed: 200, Skipped: 0\n",
      "\n",
      "Processing class: Q\n",
      " Found 200 images.\n",
      " Finished class Q. Processed: 200, Skipped: 0\n",
      "\n",
      "Processing class: R\n",
      " Found 200 images.\n",
      " Finished class R. Processed: 200, Skipped: 0\n",
      "\n",
      "Processing class: S\n",
      " Found 200 images.\n",
      " Finished class S. Processed: 200, Skipped: 0\n",
      "\n",
      "Processing class: T\n",
      " Found 200 images.\n",
      " Finished class T. Processed: 200, Skipped: 0\n",
      "\n",
      "Processing class: U\n",
      " Found 200 images.\n",
      " Finished class U. Processed: 200, Skipped: 0\n",
      "\n",
      "Processing class: V\n",
      " Found 200 images.\n",
      " Finished class V. Processed: 200, Skipped: 0\n",
      "\n",
      "Processing class: W\n",
      " Found 200 images.\n",
      " Finished class W. Processed: 200, Skipped: 0\n",
      "\n",
      "Processing class: X\n",
      " Found 200 images.\n",
      " Finished class X. Processed: 200, Skipped: 0\n",
      "\n",
      "Processing class: Y\n",
      " Found 200 images.\n",
      " Finished class Y. Processed: 200, Skipped: 0\n",
      "\n",
      "Data collection and feature extraction finished.\n",
      "Total images processed successfully: 4800\n",
      "Total images skipped: 0\n",
      "Number of data rows created: 4800\n"
     ]
    }
   ],
   "source": [
    "for class_name in class_names:\n",
    "    class_dir = os.path.join(DATA_DIR, class_name)\n",
    "    print(f\"\\nProcessing class: {class_name}\")\n",
    "\n",
    "    image_files = [f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    print(f\" Found {len(image_files)} images.\")\n",
    "\n",
    "    class_processed_count = 0\n",
    "    class_skipped_count = 0\n",
    "\n",
    "    for img_name in image_files:\n",
    "        img_path = os.path.join(class_dir, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            class_skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_rgb.flags.writeable = False\n",
    "        results = hands.process(img_rgb)\n",
    "        img_rgb.flags.writeable = True\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "\n",
    "            # --- Raw Landmark Extraction (Normalization Removed) ---\n",
    "            try:\n",
    "                # Directly extract x, y for each landmark\n",
    "                raw_landmark_coords = []\n",
    "                for lm in hand_landmarks.landmark: # Iterate through the 21 landmarks\n",
    "                    raw_landmark_coords.extend([lm.x, lm.y])\n",
    "\n",
    "                # Ensure we got the correct number of coordinates (should be 42)\n",
    "                if len(raw_landmark_coords) != NUM_LANDMARKS * 2:\n",
    "                    print(f\"  Warning: Incorrect number of coordinates ({len(raw_landmark_coords)}) extracted for {img_path}. Skipping.\")\n",
    "                    class_skipped_count += 1\n",
    "                    continue\n",
    "\n",
    "                # --- Create the row for this image ---\n",
    "                # Use the raw landmark coordinates directly\n",
    "                landmark_coords = raw_landmark_coords\n",
    "\n",
    "                # Append the class name\n",
    "                row_data = landmark_coords + [class_name]\n",
    "\n",
    "                # Add this row to our main list\n",
    "                all_rows.append(row_data)\n",
    "                class_processed_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                 print(f\"  Warning: Error during landmark extraction for {img_path}. Skipping. Error: {e}\")\n",
    "                 class_skipped_count += 1\n",
    "\n",
    "        else:\n",
    "            # No hand detected\n",
    "            class_skipped_count += 1\n",
    "\n",
    "    print(f\" Finished class {class_name}. Processed: {class_processed_count}, Skipped: {class_skipped_count}\")\n",
    "    processed_files += class_processed_count\n",
    "    skipped_files += class_skipped_count\n",
    "\n",
    "hands.close()\n",
    "\n",
    "print(\"\\nData collection and feature extraction finished.\")\n",
    "print(f\"Total images processed successfully: {processed_files}\")\n",
    "print(f\"Total images skipped: {skipped_files}\")\n",
    "print(f\"Number of data rows created: {len(all_rows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ccc9cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving data to landmark_data_raw_new.csv...\n",
      "Data saved successfully.\n",
      "\n",
      "******************************************************\n",
      "** WARNING: Data saved WITHOUT normalization.         **\n",
      "** Model performance will likely be significantly     **\n",
      "** worse compared to using normalized landmarks.      **\n",
      "******************************************************\n"
     ]
    }
   ],
   "source": [
    "# --- Save the data to CSV ---\n",
    "if all_rows:\n",
    "    print(f\"\\nSaving data to {OUTPUT_CSV_FILE}...\")\n",
    "    try:\n",
    "        with open(OUTPUT_CSV_FILE, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(headers)\n",
    "            writer.writerows(all_rows)\n",
    "        print(\"Data saved successfully.\")\n",
    "        print(\"\\n******************************************************\")\n",
    "        print(\"** WARNING: Data saved WITHOUT normalization.         **\")\n",
    "        print(\"** Model performance will likely be significantly     **\")\n",
    "        print(\"** worse compared to using normalized landmarks.      **\")\n",
    "        print(\"******************************************************\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error saving CSV file: {e}\")\n",
    "else:\n",
    "    print(\"\\nError: No data was collected or processed successfully. CSV file not saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
